{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ceab580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading X...\n",
      "X: 100,000 cells × 2,335 features\n",
      "Loaded metadata CSV: 100000 rows\n",
      "Detected 42 subclasses (using k = 42 for KMeans).\n",
      "Running PCA (200 PCs for clustering)...\n",
      "PCA done. Shape for clustering: (100000, 200)\n",
      "Explained variance ratio (first 2 PCs): 0.0298\n",
      " GMM ari: 0.3250\n",
      " GMM nmi: 0.6840\n",
      " GMM final loss: -300.5599\n",
      "Saved GMM labels.\n",
      "Macro-F1: 0.4402, Micro-F1: 0.8177\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9905    0.9874    0.9889       317\n",
      "           1     0.9081    0.7780    0.8380      1410\n",
      "           2     0.0000    0.0000    0.0000        23\n",
      "           3     0.0000    0.0000    0.0000       140\n",
      "           4     0.0000    0.0000    0.0000        20\n",
      "           5     0.0000    0.0000    0.0000       492\n",
      "           6     0.9930    0.9539    0.9730      1929\n",
      "           7     0.9996    0.9761    0.9877      5013\n",
      "           8     0.0000    0.0000    0.0000        67\n",
      "           9     0.0000    0.0000    0.0000       381\n",
      "          10     0.0000    0.0000    0.0000       176\n",
      "          11     0.9233    0.8145    0.8655     11100\n",
      "          12     0.0000    0.0000    0.0000       574\n",
      "          13     0.9667    0.7541    0.8472      4038\n",
      "          14     0.0000    0.0000    0.0000       254\n",
      "          15     0.9468    0.9301    0.9384      1130\n",
      "          16     0.9254    0.9399    0.9326       383\n",
      "          17     0.6977    0.9735    0.8129     23584\n",
      "          18     0.0000    0.0000    0.0000      4915\n",
      "          19     0.0000    0.0000    0.0000        90\n",
      "          20     0.8133    0.8779    0.8443      1523\n",
      "          21     0.0000    0.0000    0.0000       506\n",
      "          22     0.9429    0.9586    0.9507      2705\n",
      "          23     0.9792    0.8821    0.9281     12730\n",
      "          24     0.5420    0.7542    0.6307      6586\n",
      "          25     0.0000    0.0000    0.0000        84\n",
      "          26     0.7921    0.8370    0.8139      1288\n",
      "          27     0.7207    0.8643    0.7860      1791\n",
      "          28     0.9709    0.8612    0.9128      3567\n",
      "          29     0.0000    0.0000    0.0000         2\n",
      "          30     0.0000    0.0000    0.0000        83\n",
      "          31     0.0000    0.0000    0.0000       204\n",
      "          32     0.0000    0.0000    0.0000       159\n",
      "          33     0.7587    1.0000    0.8628       767\n",
      "          34     0.8716    0.9801    0.9227      2563\n",
      "          35     0.0000    0.0000    0.0000        22\n",
      "          36     0.0000    0.0000    0.0000       350\n",
      "          37     0.7669    0.7957    0.7810      1170\n",
      "          38     0.9394    0.8917    0.9149      3860\n",
      "          39     0.0000    0.0000    0.0000       166\n",
      "          40     0.0000    0.0000    0.0000        21\n",
      "          41     0.9440    0.9678    0.9558      3817\n",
      "\n",
      "    accuracy                         0.8177    100000\n",
      "   macro avg     0.4379    0.4471    0.4402    100000\n",
      "weighted avg     0.7709    0.8177    0.7860    100000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanghosh/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ryanghosh/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ryanghosh/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# PCA -> GMM (run on first N PCs) + visualization (first 2 PCs)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "# ----------------- CONFIG -----------------\n",
    "DATA_DIR = Path(\"desc_data_no_pca\")\n",
    "X_path = os.path.join(DATA_DIR, \"X_scaled_hvg.npy\")\n",
    "meta_csv = os.path.join(DATA_DIR, \"metadata_subset.csv\")\n",
    "SAMPLE_NAMES_PATH = None  # optional alignment file\n",
    "\n",
    "OUT_DIR = \"pca_gmm_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "random_state = 42\n",
    "pca_n_components_for_clustering = 200  # <-- use more PCs for GMM\n",
    "pca_n_components_for_plot = 2         # <-- for plotting\n",
    "\n",
    "# ----------------- Load data -----------------\n",
    "print(\"Loading X...\")\n",
    "X = np.load(X_path)\n",
    "n_samples, input_dim = X.shape\n",
    "print(f\"X: {n_samples:,} cells × {input_dim:,} features\")\n",
    "\n",
    "# ----------------- Load metadata -----------------\n",
    "if not os.path.exists(meta_csv):\n",
    "    raise FileNotFoundError(f\"Metadata CSV not found: {meta_csv}\")\n",
    "meta = pd.read_csv(meta_csv)\n",
    "print(f\"Loaded metadata CSV: {len(meta)} rows\")\n",
    "\n",
    "# Align metadata if SAMPLE_NAMES_PATH provided\n",
    "if SAMPLE_NAMES_PATH and os.path.exists(SAMPLE_NAMES_PATH):\n",
    "    if SAMPLE_NAMES_PATH.endswith(\".npy\"):\n",
    "        sample_names = list(np.load(SAMPLE_NAMES_PATH))\n",
    "    else:\n",
    "        with open(SAMPLE_NAMES_PATH) as f:\n",
    "            sample_names = [line.strip() for line in f if line.strip()]\n",
    "    if len(sample_names) != n_samples:\n",
    "        raise ValueError(f\"SAMPLE_NAMES length ({len(sample_names)}) != X rows ({n_samples})\")\n",
    "    meta_indexed = meta.set_index('sample_name')\n",
    "    meta = meta_indexed.loc[sample_names].reset_index()\n",
    "    print(\"Metadata aligned using SAMPLE_NAMES_PATH.\")\n",
    "\n",
    "# ----------------- Determine k -----------------\n",
    "if 'subclass_label' in meta.columns:\n",
    "    le = LabelEncoder()\n",
    "    true_labels = meta['subclass_label'].astype(str).values\n",
    "    y_int = le.fit_transform(true_labels)\n",
    "    k = len(le.classes_)\n",
    "    print(f\"Detected {k} subclasses (using k = {k} for KMeans).\")\n",
    "else:\n",
    "    true_labels = None\n",
    "    y_int = None\n",
    "    k = 10\n",
    "    print(\"No 'subclass_label' found; using default k = 10.\")\n",
    "\n",
    "# ----------------- PCA -----------------\n",
    "print(f\"Running PCA ({pca_n_components_for_clustering} PCs for clustering)...\")\n",
    "pca = PCA(n_components=pca_n_components_for_clustering, random_state=random_state)\n",
    "Z_full = pca.fit_transform(X)\n",
    "print(f\"PCA done. Shape for clustering: {Z_full.shape}\")\n",
    "print(f\"Explained variance ratio (first {pca_n_components_for_plot} PCs): {pca.explained_variance_ratio_[:pca_n_components_for_plot].sum():.4f}\")\n",
    "\n",
    "# Save PCA coords for all components\n",
    "pca_out = os.path.join(OUT_DIR, \"pca_coords_full.npy\")\n",
    "np.save(pca_out, Z_full)\n",
    "\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=k,\n",
    "    covariance_type=\"diag\",\n",
    "    max_iter=300,\n",
    "    init_params=\"kmeans\",\n",
    ")\n",
    "pred_labels = gmm.fit_predict(Z_full)\n",
    "\n",
    "ari = metrics.adjusted_rand_score(true_labels, pred_labels)\n",
    "nmi = metrics.normalized_mutual_info_score(true_labels, pred_labels)\n",
    "hom = metrics.homogeneity_score(true_labels, pred_labels)\n",
    "comp = metrics.completeness_score(true_labels, pred_labels)\n",
    "print(f\" GMM ari: {ari:.4f}\")\n",
    "print(f\" GMM nmi: {nmi:.4f}\")\n",
    "print(f\" GMM final loss: {gmm.lower_bound_:.4f}\")\n",
    "\n",
    "labels_out = os.path.join(OUT_DIR, \"gmm_labels_on_pca.npy\")\n",
    "np.save(labels_out, pred_labels)\n",
    "print(\"Saved GMM labels.\")\n",
    "\n",
    "# ----------------- Macro/Micro F1 -----------------\n",
    "if true_labels is not None:\n",
    "    df = pd.DataFrame({\"cluster\": pred_labels, \"true\": true_labels})\n",
    "    cluster_to_label = df.groupby(\"cluster\")[\"true\"].agg(lambda x: x.value_counts().index[0]).to_dict()\n",
    "    predicted_labels = np.array([cluster_to_label[c] for c in pred_labels])\n",
    "    le2 = LabelEncoder()\n",
    "    y_true_int = le2.fit_transform(true_labels)\n",
    "    y_pred_int = le2.transform(predicted_labels)\n",
    "    macro_f1 = f1_score(y_true_int, y_pred_int, average=\"macro\")\n",
    "    micro_f1 = f1_score(y_true_int, y_pred_int, average=\"micro\")\n",
    "    print(f\"Macro-F1: {macro_f1:.4f}, Micro-F1: {micro_f1:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true_int, y_pred_int, digits=4))\n",
    "\n",
    "# ----------------- Plot first 2 PCs -----------------\n",
    "# Z_plot = Z_full[:, :pca_n_components_for_plot]\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 6), constrained_layout=True)\n",
    "\n",
    "# ax = axes[0]\n",
    "# ax.scatter(Z_plot[:,0], Z_plot[:,1], c=pred_labels, s=6, cmap='tab20', linewidth=0, alpha=0.8)\n",
    "# ax.set_title(f\"PCA (GMM k={k})\")\n",
    "# ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\")\n",
    "\n",
    "# ax = axes[1]\n",
    "# if true_labels is not None:\n",
    "#     n_true = len(np.unique(y_int))\n",
    "#     ax.scatter(Z_plot[:,0], Z_plot[:,1], c=y_int, s=6, cmap='tab20', linewidth=0, alpha=0.8)\n",
    "#     ax.set_title(f\"PCA (ground truth: {n_true} subclasses)\")\n",
    "# else:\n",
    "#     ax.text(0.5,0.5,\"No ground truth\", ha='center', va='center')\n",
    "#     ax.set_title(\"Ground truth missing\")\n",
    "# ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\")\n",
    "\n",
    "# plt_path = os.path.join(OUT_DIR, \"pca_gmm_vs_truth.png\")\n",
    "# plt.savefig(plt_path, dpi=150)\n",
    "# plt.show()\n",
    "# print(\"Saved PCA figure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f6513c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m tsne = TSNE(n_components=\u001b[32m2\u001b[39m, perplexity=\u001b[32m30\u001b[39m, init=\u001b[33m'\u001b[39m\u001b[33mpca\u001b[39m\u001b[33m'\u001b[39m, learning_rate=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X_tsne = \u001b[43mtsne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m true_cat = pd.Categorical(true_labels)\n\u001b[32m     10\u001b[39m true_codes = true_cat.codes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1144\u001b[39m, in \u001b[36mTSNE.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[32m   1124\u001b[39m \n\u001b[32m   1125\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1141\u001b[39m \u001b[33;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params_vs_input(X)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_ = embedding\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1034\u001b[39m, in \u001b[36mTSNE._fit\u001b[39m\u001b[34m(self, X, skip_num_points)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[32m   1032\u001b[39m degrees_of_freedom = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_components - \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1102\u001b[39m, in \u001b[36mTSNE._tsne\u001b[39m\u001b[34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[39m\n\u001b[32m   1100\u001b[39m     opt_args[\u001b[33m\"\u001b[39m\u001b[33mmomentum\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m0.8\u001b[39m\n\u001b[32m   1101\u001b[39m     opt_args[\u001b[33m\"\u001b[39m\u001b[33mn_iter_without_progress\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.n_iter_without_progress\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m     params, kl_divergence, it = \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;66;03m# Save the final number of iterations\u001b[39;00m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = it\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:400\u001b[39m, in \u001b[36m_gradient_descent\u001b[39m\u001b[34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[32m    398\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompute_error\u001b[39m\u001b[33m\"\u001b[39m] = check_convergence \u001b[38;5;129;01mor\u001b[39;00m i == max_iter - \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m error, grad = \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m inc = update * grad < \u001b[32m0.0\u001b[39m\n\u001b[32m    403\u001b[39m dec = np.invert(inc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:281\u001b[39m, in \u001b[36m_kl_divergence_bh\u001b[39m\u001b[34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[39m\n\u001b[32m    278\u001b[39m indptr = P.indptr.astype(np.int64, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    280\u001b[39m grad = np.zeros(X_embedded.shape, dtype=np.float32)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m error = \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m c = \u001b[32m2.0\u001b[39m * (degrees_of_freedom + \u001b[32m1.0\u001b[39m) / degrees_of_freedom\n\u001b[32m    295\u001b[39m grad = grad.ravel()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Visualize clusters vs. true labels with t-SNE (also save figures)\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, init='pca', learning_rate='auto')\n",
    "X_tsne = tsne.fit_transform(Z_full)\n",
    "\n",
    "true_cat = pd.Categorical(true_labels)\n",
    "true_codes = true_cat.codes\n",
    "n_true = len(true_cat.categories)\n",
    "n_pred = pred_labels.max() + 1\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), dpi=120, sharex=True, sharey=True)\n",
    "cmap = plt.cm.get_cmap('tab20', max(n_true, n_pred))\n",
    "\n",
    "axs[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=true_codes, s=5, cmap=cmap, alpha=0.7)\n",
    "axs[0].set_title('True labels (t-SNE)')\n",
    "axs[0].set_xlabel('t-SNE 1')\n",
    "axs[0].set_ylabel('t-SNE 2')\n",
    "\n",
    "axs[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=pred_labels, s=5, cmap=cmap, alpha=0.7)\n",
    "axs[1].set_title('GMM clusters (t-SNE)')\n",
    "axs[1].set_xlabel('t-SNE 1')\n",
    "axs[1].set_ylabel('t-SNE 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plots_dir = Path('tsne_plots')\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "fig_path = plots_dir / 'tsne_true_vs_gmm.png'\n",
    "plt.savefig(fig_path, dpi=200)\n",
    "print(f'Saved t-SNE comparison plot to {fig_path}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb2dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM on noisy PCA data metrics:\n",
      "  ARI: 0.3167\n",
      "  NMI: 0.6558\n",
      "  Homogeneity: 0.7572\n",
      "  Completeness: 0.5783\n",
      " GMM noisy final loss: -363.9682\n",
      "  Silhouette: 0.0117\n",
      "Cluster sizes (noisy):\n",
      "0     1576\n",
      "1     2754\n",
      "2     5146\n",
      "3     3147\n",
      "4     6321\n",
      "5     3714\n",
      "6      988\n",
      "7     3484\n",
      "8     1146\n",
      "9     2518\n",
      "10    2212\n",
      "11    1215\n",
      "12    1375\n",
      "13    1364\n",
      "14    1923\n",
      "15    5252\n",
      "16    1895\n",
      "17    2733\n",
      "18    3334\n",
      "19    3650\n",
      "20    1634\n",
      "21    1975\n",
      "22    1820\n",
      "23    1884\n",
      "24    1606\n",
      "25     831\n",
      "26    1863\n",
      "27    1104\n",
      "28    1088\n",
      "29    2672\n",
      "30    2199\n",
      "31    1485\n",
      "32    2607\n",
      "33    3295\n",
      "34     459\n",
      "35    1594\n",
      "36     311\n",
      "37    3079\n",
      "38    4317\n",
      "39    1992\n",
      "40    3859\n",
      "41    2579\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# GMM on data with additive Gaussian noise (mean=1, var=1) in PCA space\n",
    "noise = np.random.normal(loc=1.0, scale=1.0, size=Z_full.shape)\n",
    "Z_noisy = Z_full + noise\n",
    "\n",
    "gmm_noisy = GaussianMixture(\n",
    "    n_components=k,\n",
    "    covariance_type='diag',\n",
    "    random_state=random_state,\n",
    "    max_iter=300,\n",
    "    init_params='kmeans',\n",
    "    reg_covar=1e-3,\n",
    ")\n",
    "pred_noisy = gmm_noisy.fit_predict(Z_noisy)\n",
    "\n",
    "nmi_noisy = metrics.normalized_mutual_info_score(true_labels, pred_noisy)\n",
    "ari_noisy = metrics.adjusted_rand_score(true_labels, pred_noisy)\n",
    "hom_noisy = metrics.homogeneity_score(true_labels, pred_noisy)\n",
    "comp_noisy = metrics.completeness_score(true_labels, pred_noisy)\n",
    "sil_noisy = None\n",
    "try:\n",
    "    sil_noisy = metrics.silhouette_score(Z_noisy, pred_noisy, sample_size=min(2000, len(pred_noisy)), random_state=random_state)\n",
    "except Exception as e:\n",
    "    print(f'Silhouette not computed (noisy): {e}')\n",
    "\n",
    "print('GMM on noisy PCA data metrics:')\n",
    "print(f'  ARI: {ari_noisy:.4f}')\n",
    "print(f'  NMI: {nmi_noisy:.4f}')\n",
    "print(f'  Homogeneity: {hom_noisy:.4f}')\n",
    "print(f'  Completeness: {comp_noisy:.4f}')\n",
    "print(f\" GMM noisy final loss: {gmm_noisy.lower_bound_:.4f}\")\n",
    "if sil_noisy is not None:\n",
    "    print(f'  Silhouette: {sil_noisy:.4f}')\n",
    "print('Cluster sizes (noisy):')\n",
    "print(pd.Series(pred_noisy).value_counts().sort_index())\n",
    "\n",
    "# t-SNE visualization for noisy data\n",
    "# true_cat = pd.Categorical(true_labels)\n",
    "# true_codes = true_cat.codes\n",
    "# n_true = len(true_cat.categories)\n",
    "# n_pred_noisy = pred_noisy.max() + 1\n",
    "# tsne_noisy = TSNE(n_components=2, perplexity=30, random_state=random_state, init='pca', learning_rate='auto')\n",
    "# X_tsne_noisy = tsne_noisy.fit_transform(Z_noisy)\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(14, 6), dpi=120, sharex=True, sharey=True)\n",
    "# cmap_noisy = plt.cm.get_cmap('tab20', max(n_true, n_pred_noisy))\n",
    "# axs[0].scatter(X_tsne_noisy[:, 0], X_tsne_noisy[:, 1], c=true_codes, s=5, cmap=cmap_noisy, alpha=0.7)\n",
    "# axs[0].set_title('True labels (noisy t-SNE)')\n",
    "# axs[0].set_xlabel('t-SNE 1')\n",
    "# axs[0].set_ylabel('t-SNE 2')\n",
    "\n",
    "# axs[1].scatter(X_tsne_noisy[:, 0], X_tsne_noisy[:, 1], c=pred_noisy, s=5, cmap=cmap_noisy, alpha=0.7)\n",
    "# axs[1].set_title('GMM clusters (noisy t-SNE)')\n",
    "# axs[1].set_xlabel('t-SNE 1')\n",
    "# axs[1].set_ylabel('t-SNE 2')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plots_dir = Path('tsne_plots')\n",
    "# plots_dir.mkdir(exist_ok=True)\n",
    "# noisy_fig_path = plots_dir / 'tsne_noisy_true_vs_gmm.png'\n",
    "# plt.savefig(noisy_fig_path, dpi=200)\n",
    "# print(f'Saved noisy t-SNE comparison plot to {noisy_fig_path}')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ea60642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM on partially noisy PCA data metrics:\n",
      "  ARI: 0.3020\n",
      "  NMI: 0.6549\n",
      "  Homogeneity: 0.7564\n",
      "  Completeness: 0.5774\n",
      " GMM partial noisy final loss: -351.6572\n",
      "  Silhouette: 0.0098\n",
      "Cluster sizes (partial noise):\n",
      "0     3552\n",
      "1     3336\n",
      "2     5377\n",
      "3     3977\n",
      "4     2680\n",
      "5     2522\n",
      "6     1106\n",
      "7     1162\n",
      "8     4918\n",
      "9      643\n",
      "10    2180\n",
      "11    2494\n",
      "12    1896\n",
      "13    3922\n",
      "14     999\n",
      "15     889\n",
      "16    2835\n",
      "17    1308\n",
      "18    1225\n",
      "19    3169\n",
      "20    1152\n",
      "21    1808\n",
      "22    2243\n",
      "23    1825\n",
      "24    1802\n",
      "25    5275\n",
      "26    2425\n",
      "27    2684\n",
      "28    1403\n",
      "29    1574\n",
      "30    3372\n",
      "31    1425\n",
      "32    4007\n",
      "33    1811\n",
      "34    1459\n",
      "35    1115\n",
      "36    1682\n",
      "37    2262\n",
      "38    5910\n",
      "39     801\n",
      "40    2141\n",
      "41    1634\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# GMM with partially masked Gaussian noise (50% zeros)\n",
    "noise = np.random.normal(loc=1.0, scale=1.0, size=Z_full.shape)\n",
    "mask = np.random.rand(*noise.shape) < 0.5\n",
    "noise[~mask] = 0.0\n",
    "Z_partial = Z_full + noise\n",
    "\n",
    "gmm_partial = GaussianMixture(\n",
    "    n_components=k,\n",
    "    covariance_type='diag',\n",
    "    random_state=random_state,\n",
    "    max_iter=300,\n",
    "    init_params='kmeans',\n",
    "    reg_covar=1e-3,\n",
    ")\n",
    "pred_partial = gmm_partial.fit_predict(Z_partial)\n",
    "\n",
    "nmi_partial = metrics.normalized_mutual_info_score(true_labels, pred_partial)\n",
    "ari_partial = metrics.adjusted_rand_score(true_labels, pred_partial)\n",
    "hom_partial = metrics.homogeneity_score(true_labels, pred_partial)\n",
    "comp_partial = metrics.completeness_score(true_labels, pred_partial)\n",
    "sil_partial = None\n",
    "try:\n",
    "    sil_partial = metrics.silhouette_score(Z_partial, pred_partial, sample_size=min(2000, len(pred_partial)), random_state=random_state)\n",
    "except Exception as e:\n",
    "    print(f'Silhouette not computed (partial noise): {e}')\n",
    "\n",
    "print('GMM on partially noisy PCA data metrics:')\n",
    "print(f'  ARI: {ari_partial:.4f}')\n",
    "print(f'  NMI: {nmi_partial:.4f}')\n",
    "print(f'  Homogeneity: {hom_partial:.4f}')\n",
    "print(f'  Completeness: {comp_partial:.4f}')\n",
    "print(f\" GMM partial noisy final loss: {gmm_partial.lower_bound_:.4f}\")\n",
    "if sil_partial is not None:\n",
    "    print(f'  Silhouette: {sil_partial:.4f}')\n",
    "print('Cluster sizes (partial noise):')\n",
    "print(pd.Series(pred_partial).value_counts().sort_index())\n",
    "\n",
    "# t-SNE visualization for partially noisy data\n",
    "# true_cat = pd.Categorical(true_labels)\n",
    "# true_codes = true_cat.codes\n",
    "# n_true = len(true_cat.categories)\n",
    "# n_pred_partial = pred_partial.max() + 1\n",
    "# tsne_partial = TSNE(n_components=2, perplexity=30, random_state=random_state, init='pca', learning_rate='auto')\n",
    "# X_tsne_partial = tsne_partial.fit_transform(Z_partial)\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(14, 6), dpi=120, sharex=True, sharey=True)\n",
    "# cmap_partial = plt.cm.get_cmap('tab20', max(n_true, n_pred_partial))\n",
    "# axs[0].scatter(X_tsne_partial[:, 0], X_tsne_partial[:, 1], c=true_codes, s=5, cmap=cmap_partial, alpha=0.7)\n",
    "# axs[0].set_title('True labels (partial-noise t-SNE)')\n",
    "# axs[0].set_xlabel('t-SNE 1')\n",
    "# axs[0].set_ylabel('t-SNE 2')\n",
    "\n",
    "# axs[1].scatter(X_tsne_partial[:, 0], X_tsne_partial[:, 1], c=pred_partial, s=5, cmap=cmap_partial, alpha=0.7)\n",
    "# axs[1].set_title('GMM clusters (partial-noise t-SNE)')\n",
    "# axs[1].set_xlabel('t-SNE 1')\n",
    "# axs[1].set_ylabel('t-SNE 2')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plots_dir = Path('tsne_plots')\n",
    "# plots_dir.mkdir(exist_ok=True)\n",
    "# partial_fig_path = plots_dir / 'tsne_partial_noise_true_vs_gmm.png'\n",
    "# plt.savefig(partial_fig_path, dpi=200)\n",
    "# print(f'Saved partial-noise t-SNE comparison plot to {partial_fig_path}')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dfd8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SFAA\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "\n",
    "def compute_pairwise_cluster_indicator(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build an n x n matrix M where M[i, j] = 1 if labels[i] == labels[j], else 0.\n",
    "    This corresponds to Y Y^T in the paper.\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    n = labels.shape[0]\n",
    "    M = np.zeros((n, n), dtype=np.int8)\n",
    "    for k in np.unique(labels):\n",
    "        idx = np.where(labels == k)[0]\n",
    "        M[np.ix_(idx, idx)] = 1\n",
    "    return M\n",
    "\n",
    "\n",
    "def evaluate_spillover(\n",
    "    X: np.ndarray,\n",
    "    clusterer,\n",
    "    base_labels: np.ndarray,\n",
    "    base_indicator: np.ndarray,\n",
    "    pivot_idx: int,\n",
    "    delta: np.ndarray,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a perturbation delta to X[pivot_idx], re-run clustering, and compute:\n",
    "    - new labels\n",
    "    - spillover score: number of pairwise label-relationship changes\n",
    "\n",
    "    Spillover score is: || Y Y^T - Y' Y'^T ||_F^2 (we can use sum of squared diff).\n",
    "    \"\"\"\n",
    "    X_pert = X.copy()\n",
    "    X_pert[pivot_idx] = X[pivot_idx] + delta\n",
    "\n",
    "    # Re-fit clusterer (black box)\n",
    "    cl = clone(clusterer)\n",
    "    pert_labels = cl.fit_predict(X_pert)\n",
    "\n",
    "    pert_indicator = compute_pairwise_cluster_indicator(pert_labels)\n",
    "\n",
    "    diff = base_indicator - pert_indicator\n",
    "    # Frobenius norm squared of the difference:\n",
    "    spill = np.sum(diff ** 2)\n",
    "\n",
    "    return spill, pert_labels, X_pert\n",
    "\n",
    "\n",
    "def pick_pivot_point(\n",
    "    X: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    k1: int,\n",
    "    k2: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Among points in cluster k1, pick the one closest to the centroid of cluster k2.\n",
    "    This is the 'most boundary-like' point along k1->k2 direction.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    k1_idx = np.where(labels == k1)[0]\n",
    "    k2_idx = np.where(labels == k2)[0]\n",
    "\n",
    "    if len(k1_idx) == 0 or len(k2_idx) == 0:\n",
    "        raise ValueError(\"One of the clusters is empty; cannot pick pivot.\")\n",
    "\n",
    "    c2 = X[k2_idx].mean(axis=0)\n",
    "\n",
    "    # index (in k1_idx) of the point closest to c2\n",
    "    local_best = np.argmin(np.linalg.norm(X[k1_idx] - c2, axis=1))\n",
    "    pivot_idx = k1_idx[local_best]\n",
    "    return pivot_idx\n",
    "\n",
    "\n",
    "def random_delta(d: int, delta_max: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sample a random perturbation in [-delta_max, delta_max] elementwise.\n",
    "    delta_max can be scalar or vector of length d.\n",
    "    \"\"\"\n",
    "    delta_max = np.asarray(delta_max)\n",
    "    if delta_max.size == 1:\n",
    "        delta_max = np.full(d, float(delta_max))\n",
    "    return np.random.uniform(-delta_max, delta_max, size=d)\n",
    "\n",
    "\n",
    "def attack_pair(\n",
    "    X: np.ndarray,\n",
    "    clusterer,\n",
    "    k1: int,\n",
    "    k2: int,\n",
    "    delta_max,\n",
    "    n_iters: int = 100,\n",
    "    n_candidates_per_iter: int = 10,\n",
    "    random_state: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a one-point black-box attack targeting cluster k1 -> k2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (n, d) array\n",
    "        Data matrix.\n",
    "    clusterer : sklearn-like estimator\n",
    "        Any clustering algorithm with fit_predict(X).\n",
    "    k1 : int\n",
    "        Source cluster index (where pivot point starts).\n",
    "    k2 : int\n",
    "        Target cluster index.\n",
    "    delta_max : float or (d,) array\n",
    "        Max perturbation magnitude per feature (L_infinity ball).\n",
    "    n_iters : int\n",
    "        Number of optimization iterations.\n",
    "    n_candidates_per_iter : int\n",
    "        Random candidates per iteration.\n",
    "    random_state : int or None\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_result : dict\n",
    "        Contains:\n",
    "            - 'pivot_idx'\n",
    "            - 'best_delta'\n",
    "            - 'best_spill'\n",
    "            - 'base_labels'\n",
    "            - 'adv_labels'\n",
    "            - 'X_adv'\n",
    "            - 'k1', 'k2'\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Fit base clustering once\n",
    "    cl0 = clone(clusterer)\n",
    "    base_labels = cl0.fit_predict(X)\n",
    "    base_indicator = compute_pairwise_cluster_indicator(base_labels)\n",
    "\n",
    "    # Pick pivot point in k1 closest to centroid of k2\n",
    "    pivot_idx = pick_pivot_point(X, base_labels, k1, k2)\n",
    "    d = X.shape[1]\n",
    "\n",
    "    best_spill = -np.inf\n",
    "    best_delta = np.zeros(d)\n",
    "    best_labels = base_labels\n",
    "    best_X_adv = X\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        for _ in range(n_candidates_per_iter):\n",
    "            delta = random_delta(d, delta_max)\n",
    "            spill, pert_labels, X_pert = evaluate_spillover(\n",
    "                X, clusterer, base_labels, base_indicator, pivot_idx, delta\n",
    "            )\n",
    "\n",
    "            if spill > best_spill:\n",
    "                best_spill = spill\n",
    "                best_delta = delta\n",
    "                best_labels = pert_labels\n",
    "                best_X_adv = X_pert\n",
    "\n",
    "    return {\n",
    "        \"pivot_idx\": pivot_idx,\n",
    "        \"best_delta\": best_delta,\n",
    "        \"best_spill\": best_spill,\n",
    "        \"base_labels\": base_labels,\n",
    "        \"adv_labels\": best_labels,\n",
    "        \"X_adv\": best_X_adv,\n",
    "        \"k1\": k1,\n",
    "        \"k2\": k2,\n",
    "    }\n",
    "\n",
    "\n",
    "def attack_best_pair(\n",
    "    X: np.ndarray,\n",
    "    clusterer,\n",
    "    delta_max,\n",
    "    n_iters_per_pair: int = 50,\n",
    "    n_candidates_per_iter: int = 10,\n",
    "    random_state: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Search over all ordered cluster pairs (k1, k2) with k1 != k2 and\n",
    "    return the best attack.\n",
    "\n",
    "    This is the K-cluster extension: we don't assume only 2 clusters exist.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    cl0 = clone(clusterer)\n",
    "    base_labels = cl0.fit_predict(X)\n",
    "    unique_clusters = np.unique(base_labels)\n",
    "\n",
    "    best_global = None\n",
    "    best_global_spill = -np.inf\n",
    "\n",
    "    for k1 in unique_clusters:\n",
    "        for k2 in unique_clusters:\n",
    "            if k1 == k2:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = attack_pair(\n",
    "                    X,\n",
    "                    clusterer,\n",
    "                    k1=int(k1),\n",
    "                    k2=int(k2),\n",
    "                    delta_max=delta_max,\n",
    "                    n_iters=n_iters_per_pair,\n",
    "                    n_candidates_per_iter=n_candidates_per_iter,\n",
    "                    random_state=rng.integers(1e9),\n",
    "                )\n",
    "            except ValueError:\n",
    "                # might happen if cluster is empty for some reason\n",
    "                continue\n",
    "\n",
    "            if result[\"best_spill\"] > best_global_spill:\n",
    "                best_global_spill = result[\"best_spill\"]\n",
    "                best_global = result\n",
    "\n",
    "    return best_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run SFAA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gcp-venv)",
   "language": "python",
   "name": "gcp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
